{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Challenge 1: The banknote-authentication data set problem",
      "metadata": {
        "id": "a7085776"
      },
      "id": "a7085776"
    },
    {
      "cell_type": "markdown",
      "source": "We will perform a nearly realistic analysis of the data set bank note authentication that can be downloaded from https://archive-beta.ics.uci.edu/dataset/267/banknote+authentication",
      "metadata": {
        "id": "7470076a"
      },
      "id": "7470076a"
    },
    {
      "cell_type": "markdown",
      "source": "## Data set description\n\nData were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\nThese features are:\n1. variance of Wavelet Transformed image (continuous) \n2. skewness of Wavelet Transformed image (continuous) \n3. curtosis of Wavelet Transformed image (continuous) \n4. entropy of image (continuous) \n5. class (integer)",
      "metadata": {
        "id": "d7d1976d"
      },
      "id": "d7d1976d"
    },
    {
      "cell_type": "markdown",
      "source": "## Task description\nWe have a binary classification problem. The assignment can be divided in several parts:\n    \n    1. Load the data and pretreatment.\n    2. Data exploring by Unsupervised Learning techniques.\n    3. Construction of several models of Supervised Learning.",
      "metadata": {
        "id": "958852e7"
      },
      "id": "958852e7"
    },
    {
      "cell_type": "markdown",
      "source": "### 1. Data pretreatment\n\nLoad the data and look at it: It is needed some kind of scaling? Why? Are the data points sorted in the original data set? Can it generate problems? How can this be solved?",
      "metadata": {
        "id": "feabe205"
      },
      "id": "feabe205"
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix",
      "metadata": {
        "id": "Cf2boXBSzPkA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Cf2boXBSzPkA"
    },
    {
      "cell_type": "code",
      "source": "colNames=['variance','skewness','curtosis','entropy','counterfit']\ndf = pd.read_csv('./data_banknote_authentication.txt', header=None, names=colNames)\ndf",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "d8b90dd7"
    },
    {
      "cell_type": "code",
      "source": "df.isna().sum()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "f1561b5b"
    },
    {
      "cell_type": "code",
      "source": "df_shuffled = df.sample(frac=1,random_state=123).reset_index(drop=True)\ndf_shuffled",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "01453cc9"
    },
    {
      "cell_type": "code",
      "source": "y = df_shuffled[\"counterfit\"].copy()\nX = df_shuffled.drop(columns=[\"counterfit\"])\nX.shape",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "59945818"
    },
    {
      "cell_type": "code",
      "source": "import seaborn as sb\n\nplt.rcParams['figure.figsize']=15,5\nplt.subplot(121)\nplt.title('Banknote Class Type Count', fontsize=10)\ns = sb.countplot(x = \"counterfit\", data = df, alpha=0.7)\nfor p in s.patches:\n    s.annotate(format(p.get_height(), '.1f'), \n               (p.get_x() + p.get_width() / 2., p.get_height()), \n                ha = 'center', va = 'center', \n                xytext = (0, 4), \n                textcoords = 'offset points')\n\nax = plt.subplot(122)\nclasspie = df['counterfit'].value_counts()\nsize = classpie.values.tolist()\ntypes = classpie.axes[0].tolist()\nlabels = 'Yes', 'No'\ncolors = ['#EAFFD0', '#F38181']\nplt.title('Banknote Class Type Percentange', fontsize=10)\npatches, texts, autotexts = plt.pie(size, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=150)\nfor text,autotext in zip(texts,autotexts):\n    text.set_fontsize(14)\n    autotext.set_fontsize(14)\n\nplt.axis('equal')",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "b403a972"
    },
    {
      "cell_type": "code",
      "source": "from sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler().fit(X)\nX_scaled = scaler.transform(X)\nX_scaled = pd.DataFrame(X_scaled, columns=['variance','skewness','curtosis','entropy'])\nX_scaled",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "25e1b6bd"
    },
    {
      "cell_type": "markdown",
      "source": "### Analisi esplorativa dei dati",
      "metadata": {},
      "id": "4348c128"
    },
    {
      "cell_type": "code",
      "source": "ax = X_scaled.hist(figsize=(20,10))\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "e69f6723"
    },
    {
      "cell_type": "code",
      "source": "ax = X_scaled.boxplot(figsize=(20,10))\nax.set_xlabel('Features')\nax.set_ylabel('Values')\nax.set_title('Outliers detection')\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "1d6dda2b"
    },
    {
      "cell_type": "code",
      "source": "pd.plotting.scatter_matrix(X_scaled, c = y, figsize = (18,18), diagonal = \"kde\", alpha = .8)\nplt.suptitle(\"UCI Dataset Scatter Matrix\", y = .9)\nplt.show()\nplt.close()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "9cacf886"
    },
    {
      "cell_type": "markdown",
      "source": "### 2. Unsupervised Learning\n\nUse PCA and plot the two first components colouring according with the class. Are the classes linearly separable in this projection? What happens when I applied k-means with two classes in this space? And if I use all the coordinates? Try also t-SNE for projection and DBSCAN for the clustering and comment on the results.",
      "metadata": {
        "id": "4961a5b9"
      },
      "id": "4961a5b9"
    },
    {
      "cell_type": "markdown",
      "source": "#### PCA",
      "metadata": {},
      "id": "f3f0f220"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.decomposition import PCA\npca = PCA()\npca.fit(X_scaled)\nX_pca = pca.transform(X_scaled)\n\nperc_var = np.round(pca.explained_variance_ratio_*100, decimals=1)\nlabels = ['PC' + str(x) for x in range(1, len(perc_var)+1)]\n\nplt.bar(x=range(1, len(perc_var)+1), height=perc_var, tick_label=labels)\nplt.ylabel('Percentage of Explained Variance')\nplt.xlabel('Principal Component')\nplt.title('Scree Plot')\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "0ffd386c"
    },
    {
      "cell_type": "code",
      "source": "pca_df = pd.DataFrame(X_pca, columns=labels)\nplt.scatter(pca_df.PC1, pca_df.PC2, c=y)\nplt.title('PCA Graph')\nplt.xlabel('PC1 - {0}%'.format(perc_var[0]))\nplt.ylabel('PC2 - {0}%'.format(perc_var[1]))\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "e3972be8"
    },
    {
      "cell_type": "markdown",
      "source": "#### K-means\n\nWhat happens when I applied k-means with two classes in this space? And if I use all the coordinates?",
      "metadata": {},
      "id": "6619792e"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2)\n\nkmeans.fit(X_pca)\n\nkmeans_labels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels)\nplt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300, c='r')\nplt.xlabel('Prima componente principale')\nplt.ylabel('Seconda componente principale')\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "ceb4951a"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics.cluster import normalized_mutual_info_score\n\nnormalized_mutual_info_score(kmeans_labels, np.array(y).flatten())",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "c46bc085"
    },
    {
      "cell_type": "markdown",
      "source": "#### t-SNE\n\nTry also t-SNE for projection and DBSCAN for the clustering and comment on the results.",
      "metadata": {},
      "id": "65a2739b"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.manifold import TSNE\nX_embedded = TSNE(n_components=2, learning_rate='auto',\n                  init='random', perplexity=15, random_state=42).fit_transform(X_scaled)\nfig, ax =plt.subplots(figsize=(9,9))\nax.scatter(X_embedded[:,0],X_embedded[:,1], c=y)\nax.set_title('t-SNE')\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "2395bfad"
    },
    {
      "cell_type": "markdown",
      "source": "#### DBSCAN",
      "metadata": {},
      "id": "04fdbb35"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=0.7, min_samples=15).fit(X_pca)\n\nfig = plt.figure(figsize=(9, 9))\nax = fig.add_subplot()\nax.scatter(pca_df.PC1, pca_df.PC2,c=dbscan.labels_)\n\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "0b1192e0"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=0.7, min_samples=12).fit(X_pca)\n\nfig = plt.figure(figsize=(9, 9))\nax = fig.add_subplot()\nax.scatter(pca_df.PC1, pca_df.PC2,c=dbscan.labels_)\n\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "dc1615e4"
    },
    {
      "cell_type": "markdown",
      "source": "### 3. Supervised Learning\n\nGenerate a subset of the data of 372 elements that would be saved as test set. With the rest of the data generate the following models: Logistic Regression, Decision tree (use the ID3 algorithm), Naive Bayesian and k-NN. \n\nInvestigate the effect of regularization (when possible) and use cross validation for setting the hyper-parameters when needed. \n\nCompare the performances in terms of accuracy, precision, recall and F1-score on the test set. Comment these results at the light of those obtained from the Unsupervised Learning analysis. Could you propose a way to improve these results?     \n",
      "metadata": {
        "id": "3b00d564"
      },
      "id": "3b00d564"
    },
    {
      "cell_type": "code",
      "source": "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2711, stratify=y, random_state=2)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "8985efea"
    },
    {
      "cell_type": "markdown",
      "source": "#### TEST",
      "metadata": {},
      "id": "7456db00"
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\n\nimport sklearn.metrics as metric",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "557b09ae"
    },
    {
      "cell_type": "code",
      "source": "estimators = {\n    'LogisticRegression': LogisticRegression(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(n_neighbors=5),\n    'SVM': SVC(),\n    'XGB': xgb.XGBClassifier()\n}",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "4614def2"
    },
    {
      "cell_type": "code",
      "source": "def train_model(estimator, X_train, X_test, y_train, y_test):\n    estimator.fit(X_train, y_train)\n    y_pred = estimator.predict(X_test)\n    print(f'The accuracy score is: {metric.accuracy_score(y_test, y_pred):.4f}')\n    print(f'The report is: {metric.classification_report(y_test, y_pred)}')\n    print('#'*100)\n    \ndef estimator_dict(X_train, X_test, y_train, y_test):\n    for name, estimator in estimators.items():\n        print(name)\n        train_model(estimator, X_train, X_test, y_train, y_test)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "4b1e27f8"
    },
    {
      "cell_type": "code",
      "source": "estimator_dict(X_tr, X_val, y_tr, y_val)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "f37e49d0"
    },
    {
      "cell_type": "markdown",
      "source": "### LOGISTIC REGRESSION",
      "metadata": {},
      "id": "6617722f"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2711, random_state=2)\n\nlr = LogisticRegression()\n\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\n\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\ny_prob = lr.predict_proba(X_test)[:,1]\nfpr, tpr, _ = roc_curve(y_test, y_prob)\nauc = roc_auc_score(y_test, y_prob)\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (AUC={:.2f})'.format(auc))\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "bfeafb20"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nlogistic_regression_model = LogisticRegression(penalty='l2')\n\nparam_grid = {'C': [0.1, 1, 10, 100]}\ngrid_search = GridSearchCV(logistic_regression_model, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\nprint('Best parameter:', grid_search.best_params_)\nprint('Best score:', grid_search.best_score_)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "9efb2857"
    },
    {
      "cell_type": "markdown",
      "source": "#### DECISION TREE",
      "metadata": {},
      "id": "b5e54c4b"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ndtc = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n\ndtc.fit(X_train, y_train)\n\ny_pred = dtc.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nprint(classification_report(y_test, y_pred))",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "534a632c"
    },
    {
      "cell_type": "markdown",
      "source": "#### Regolarizzazione e Cross Validation",
      "metadata": {},
      "id": "16fab3a3"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\ntree = DecisionTreeClassifier()\n\nparam_grid = {'max_depth': range(1, 11)}\ngrid_search = GridSearchCV(tree, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\nprint('Best parameter:', grid_search.best_params_)\nprint('Best score:', grid_search.best_score_)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "92d47ff9"
    },
    {
      "cell_type": "code",
      "source": "best_tree = DecisionTreeClassifier(max_depth=grid_search.best_params_['max_depth'])\nbest_tree.fit(X_train, y_train)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "f995a85c"
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(15,7.5))\nplot_tree(best_tree,\n          filled=True,\n          rounded=True,\n          class_names=['Edible','Poisonous'],\n          feature_names=colNames)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "56891c44"
    },
    {
      "cell_type": "markdown",
      "source": "#### NAIVE BAYESIAN",
      "metadata": {},
      "id": "29b7daaf"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nnb = GaussianNB()\n\nnb.fit(X_train, y_train)\n\ny_pred = nb.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\nprint(classification_report(y_test, y_pred))",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "aa581ab8"
    },
    {
      "cell_type": "markdown",
      "source": "#### K-NN",
      "metadata": {},
      "id": "34492eb3"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nknn = KNeighborsClassifier(n_neighbors=5)\n\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "acdb273d"
    },
    {
      "cell_type": "markdown",
      "source": "#### Cross Validation",
      "metadata": {},
      "id": "e025bf79"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nknn = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15]}\n\ngrid_search = GridSearchCV(knn, param_grid=param_grid, cv=5)\n\ngrid_search.fit(X_train, y_train)\n\nbest_n_neighbors = grid_search.best_params_['n_neighbors']\n\nknn_best = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n\nknn_best.fit(X_train, y_train)\n\nknn_best_score = knn_best.score(X_test, y_test)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "c19e127a"
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score\n\nknn_model = KNeighborsClassifier(n_neighbors=5)\n\ncv_results = cross_validate(knn_model, X, y, cv=5, scoring='accuracy')\n\nprint('Accuracy media: ', cv_results['test_score'].mean())\nprint('Deviazione standard: ', cv_results['test_score'].std())",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "f41496bd"
    }
  ]
}